import uuid
from fastapi import APIRouter
from pydantic import BaseModel
from typing import Literal, Optional
from fastapi.responses import StreamingResponse
# from langchain.memory import ConversationBufferWindowMemory
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
# from app.routers.search import perform_search
from app.managers.local_llm_manager import llm_manager
from fastapi import APIRouter, HTTPException
from app.managers.db_manager import db_manager, get_unique_parent_id_with_score, search_chunk_by_parent_id
from app.managers.memory_manager import MEMORY_WINDOW
from app.managers import memory_manager  # ‡πÉ‡∏ä‡πâ global instance ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
from app.prompts.classifier_prompts import build_classification_prompt
from app.prompts.expert_prompts import INSTRUCTIONS, FALLBACK_INSTRUCTION, base_prompt

router = APIRouter()

# ------------------------------
# Config
# ------------------------------
SUMMARY_TRIGGER_LEN = 12  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô message ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡∏∏‡∏õ
# MEMORY_WINDOW = 12  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô message ‡∏ó‡∏µ‡πà‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô memory

# ------------------------------
# In-memory stores
# ------------------------------
# memory_store: dict[str, dict[str, ConversationBufferWindowMemory]] = {}
# memory_stores: dict[str, dict[str, ConversationBufferWindowMemory]] = {}
# summary_store: dict[str, dict[str, str]] = {}  # summary_store[user_id][mode] = str

# ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á: memory_stores[user_id][chat_session_id][mode] = memory
# memory_stores: dict[str, dict[str, dict[str, ConversationBufferWindowMemory]]] = {}
# ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á: summary_store[user_id][chat_session_id][mode] = summary_str
# summary_store: dict[str, dict[str, dict[str, str]]] = {}

# ‡∏™‡∏£‡πâ‡∏≤‡∏á instance In-memory stores

# ------------------------------
# Request model
# ------------------------------
class SearchRequest(BaseModel):
    prompt: str
    # collection_name: str = "thai_law_hybrid"
    l_search: int = 15
    l_chunk: int = 20
    model: str | None = "llama3.2"
    user_id: str | None = None
    chat_session_id: str | None = None

# ------------------------------
# Memory functions
# ------------------------------

# --------- memory ‡∏´‡∏•‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ (‡πÉ‡∏ô-memory store) ---------
# def get_memory(user_id: str) -> ConversationBufferWindowMemory:
#     if user_id not in memory_store:
#         memory_store[user_id] = ConversationBufferWindowMemory(k=5, return_messages=True)
#     return memory_store[user_id]

# def get_memories(user_id: str, mode: Literal["general", "welfare", "law", "unknown"]) -> ConversationBufferWindowMemory:
#     if user_id not in memory_stores:
#         memory_stores[user_id] = {}
#     if mode not in memory_stores[user_id]:
#         memory_stores[user_id][mode] = ConversationBufferWindowMemory(k=MEMORY_WINDOW, return_messages=True)
#     return memory_stores[user_id][mode]

# def get_summary(user_id: str, mode: str) -> str:
#     return summary_store.get(user_id, {}).get(mode, "")

# def set_summary(user_id: str, mode: str, summary: str):
#     if user_id not in summary_store:
#         summary_store[user_id] = {}
#     summary_store[user_id][mode] = summary

# def get_memories(
#     user_id: str,
#     chat_session_id: str,
#     mode: Literal["general", "welfare", "law", "unknown"]
# ) -> ConversationBufferWindowMemory:
#     if user_id not in memory_stores:
#         memory_stores[user_id] = {}
#     if chat_session_id not in memory_stores[user_id]:
#         memory_stores[user_id][chat_session_id] = {}
#     if mode not in memory_stores[user_id][chat_session_id]:
#         memory_stores[user_id][chat_session_id][mode] = ConversationBufferWindowMemory(
#             k=MEMORY_WINDOW,
#             return_messages=True
#         )
#     return memory_stores[user_id][chat_session_id][mode]

# def get_summary(user_id: str, chat_session_id: str, mode: str) -> str:
#     return summary_store.get(user_id, {}).get(chat_session_id, {}).get(mode, "")

# def set_summary(user_id: str, chat_session_id: str, mode: str, summary: str):
#     if user_id not in summary_store:
#         summary_store[user_id] = {}
#     if chat_session_id not in summary_store[user_id]:
#         summary_store[user_id][chat_session_id] = {}
#     summary_store[user_id][chat_session_id][mode] = summary

# def set_prompt_expert(expert: str, context: str, history_str: str, user_question: str):
#     prompt_rag = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô{expert} ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å{expert}‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô

# ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:
# {context}

# ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:
# {history_str}

# ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {user_question}
# ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:

# ‡∏´‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ç‡∏≠ "‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°" ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô RAG ‡πÄ‡∏ä‡πà‡∏ô ‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏•‡∏≤‡∏û‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏ô ‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏•‡∏≤‡∏Å‡∏¥‡∏à ‡∏Ø‡∏•‡∏Ø  
# ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Markdown link ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô:

# üìé [‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏•‡∏≤‡∏û‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏ô](https://yourdomain.com/forms/leave_form_vacation.pdf)

# ‡∏≠‡∏¢‡πà‡∏≤‡πÅ‡∏™‡∏î‡∏á URL ‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô `https://...` ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå

# ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏ä‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
# ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÉ‡∏î‡πÜ ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô url ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡πÄ‡∏à‡∏≠‡πÄ‡∏•‡∏¢
# """
#     return prompt_rag

def set_prompt_expert(expert: str, context: str, history: str, question: str) -> str:
    """
    ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å instruction ‡∏ï‡∏≤‡∏° expert ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏ú‡πà‡∏≤‡∏ô base template
    ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å expert ‡∏à‡∏∞ fallback ‡πÄ‡∏õ‡πá‡∏ô generic helper
    """
    instruction = INSTRUCTIONS.get(expert, FALLBACK_INSTRUCTION)
    expert_name = expert if expert in INSTRUCTIONS else "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"
    
    return base_prompt(expert_name, instruction, context, history, question)

# ------------------------------
# Dummy classify function
# ------------------------------
GREETING_KEYWORDS = ["‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", "‡∏ö‡∏≤‡∏¢", "‡∏ó‡∏î‡∏™‡∏≠‡∏ö", "hello", "hi", "‡∏Ñ‡∏∏‡∏ì‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£"]

def is_general_question(prompt: str) -> bool:
    return any(word in prompt.lower() for word in GREETING_KEYWORDS)

async def classify_question_type(prompt: str) -> Literal["‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ", "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢", "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£", "‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°"]:
    
    if (is_general_question(prompt)):
        return "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"
    
    few_shot_prompt = build_classification_prompt(prompt)

    result = ""
    async for chunk in llm_manager.llm.astream(few_shot_prompt):
        result += chunk

    if "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ" in result:
        return "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"
    elif "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£" in result:
        return "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£"
    else:
        return "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢"


# ------------------------------
# Summarization function
# ------------------------------
async def summarize_history(history: list) -> str:
    conversation = "\n".join([
        f"User: {msg.content}" if isinstance(msg, HumanMessage) else f"AI: {msg.content}"
        for msg in history
    ])
    prompt = f"""‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö:\n\n{conversation}\n\n‡∏™‡∏£‡∏∏‡∏õ:"""

    print('Prompt : ', prompt)
    
    summary = ""
    async for chunk in llm_manager.llm.astream(prompt):
        summary += chunk

    print('\nSummary : ', summary.strip())

    return summary.strip()

# ------------------------------
# Perform Search function
# ------------------------------
async def perform_search(prompt: str, collection_name: str, l_search: int, l_chunk: int):
    try:
        vectorstore = db_manager.get_vectorstore(collection_name)

        # Step 1: ‡∏î‡∏∂‡∏á top 15 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏≤‡∏à‡∏°‡∏µ doc ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥ ‡∏î‡∏∂‡∏á‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô
        result_with_score = vectorstore.similarity_search_with_score(prompt, k=l_search)
        # Step 2: ‡∏Ñ‡∏±‡∏î doc ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô (l_search) ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô ‡∏Å‡∏£‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ parent_id
        unique_docs = get_unique_parent_id_with_score(result_with_score, limit=l_search)

        contexts = []
        for parent_id, chunk in unique_docs.items():
            full_content = search_chunk_by_parent_id(collection_name=collection_name, parent_id=parent_id, limit=l_chunk)
            score = chunk[0][1]
            contexts.append({
                "parent_id": parent_id,
                "score": score,
                "content": full_content
            })

        # print(db_manager.get_embedding("dense").embed_query("‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"))

        return contexts

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {str(e)}"
        )

# ------------------------------
# Match Form function
# ------------------------------    
def match_form_from_keywords(prompt: str) -> Optional[str]:
    prompt = prompt.lower()
    forms = {
        "‡∏•‡∏≤‡∏û‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏ô": "forms/leave_form_vacation.pdf",
        "‡∏•‡∏≤‡∏Å‡∏¥‡∏à": "forms/leave_form_personal.pdf",
        "‡∏•‡∏≤‡∏õ‡πà‡∏ß‡∏¢": "forms/leave_form_sick.pdf",
        "‡∏•‡∏≤‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô": "forms/leave_form_sick.pdf"
    }
    for keyword, path in forms.items():
        if keyword in prompt and ("‡πÅ‡∏ö‡∏ö" in prompt or "‡∏ü‡∏≠‡∏£‡πå‡∏°" in prompt):
            return f"http://localhost:8000/{path}", keyword
    return None

def markdown_link_from_url(url: str, label: str = "‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î") -> str:
    return f"[{label}]({url})"


# ------------------------------
# Main endpoint
# ------------------------------
@router.post("/llm/search-rag-category/chat")
async def search_endpoint(request: SearchRequest):
    if request.model:
        llm_manager.set_model(request.model)

    user_id = request.user_id or str(uuid.uuid4())  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î user id ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á‡∏°‡∏≤
    chat_session_id = request.chat_session_id or str(uuid.uuid4())  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î chat_session_id ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á‡∏°‡∏≤
    user_question = request.prompt

    # ------- Classify ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô ------
    category = await classify_question_type(user_question)

    print("‡∏´‡∏°‡∏ß‡∏î : ", category)

    # ‡πÅ‡∏¢‡∏Å memory
    if category == "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ":
        mode = "general"
    elif category == "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£":
        mode = "welfare"
    elif category == "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢":
        mode = "law"
    else:
        mode = "unknown"

    # category = "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"
    
    # memory = get_memory(user_id)
    memory = memory_manager.get_memories(user_id, chat_session_id, mode)

    # Load history & summary
    summary = memory_manager.get_summary(user_id, chat_session_id, mode)

    history = memory.load_memory_variables({})["history"]

    # ------- Load ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å memory ------
    history_str = ""
    
    for msg in history:
        if isinstance(msg, HumanMessage):
            history_str += f"User: {msg.content}\n"
        elif isinstance(msg, AIMessage):
            history_str += f"AI: {msg.content}\n"

    # ‡πÉ‡∏™‡πà summary ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô prompt ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if summary:
        history_str = f"‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤:\n{summary}\n\n{history_str}"

    # ------- ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ RAG) ------
    if category == "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ":
        # print('summary-general-memory : ', summary)
        # print('history-general-memory : ', history)
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏†‡∏≤‡∏û\n\n‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:\n\n{history_str}\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {user_question}
\n\n‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
**‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏£‡∏£‡∏û‡∏ô‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏Ñ‡∏£‡∏±‡∏ö" ‡πÄ‡∏™‡∏°‡∏≠ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ "‡∏Ñ‡πà‡∏∞" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞"**"""
        full_response = ""
        async def stream_general():
            nonlocal full_response
            print("Start streaming general response")
            try:
                async for chunk in llm_manager.llm.astream(prompt):
                    full_response += chunk
                    yield chunk
            except Exception as e:
                print(f"Exception during streaming: {e}")
            finally:
                try:
                    await finalize()
                except Exception as e:
                    print("‚ùå Error in finalize_rag:", e)
            print("Finished streaming general response")

        async def finalize():
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô memory
            memory.chat_memory.add_user_message(user_question)
            memory.chat_memory.add_ai_message(full_response)

            history_now = memory.load_memory_variables({})["history"]

            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏∏‡∏î trigger ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
            if len(history_now) >= SUMMARY_TRIGGER_LEN:
                to_summarize = history_now[:-2]
                new_summary = await summarize_history(to_summarize)

                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á summary store
                memory_manager.set_summary(
                    user_id=user_id,
                    chat_session_id=chat_session_id,
                    mode=mode,
                    summary=new_summary
                )

                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï SystemMessage ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô summary ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏•‡∏ö history ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                msgs = memory.chat_memory.messages
                summary_index = next(
                    (i for i, m in enumerate(msgs) if isinstance(m, SystemMessage) and m.content.startswith("‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")),
                    None
                )

                if summary_index is not None:
                    msgs[summary_index] = SystemMessage(content=f"‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤:\n{new_summary}")
                else:
                    msgs.insert(0, SystemMessage(content=f"‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤:\n{new_summary}"))

                memory.chat_memory.messages = msgs

                # his_to = memory.load_memory_variables({})["history"]
                # print('‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ : ', his_to)

        # print('instance memory : ',id(memory_stores[user_id]["general"]))
        # print('‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ RAG : ', history_str)
        return StreamingResponse(stream_general(), media_type="text/plain")

    # ------- ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡πÉ‡∏ä‡πâ RAG) -------

    if (category == "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢") :
        collection_name = "thai_law_hybrid"
    elif (category == "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£"):
        collection_name = "welfare"
    else:
        collection_name = "unknown"
        
    data_from_rag = await perform_search(
        prompt=request.prompt, 
        collection_name=collection_name, 
        l_search=request.l_search, 
        l_chunk=request.l_chunk
    )
    filtered_contents = [item["content"] for item in data_from_rag if item["score"] > 0.2]
    context = "\n".join(filtered_contents)

    prompt_rag = set_prompt_expert(category, context, history_str, user_question)

    final_answer = ""
    async def stream_rag():
        nonlocal final_answer
        print("Start streaming rag response")
        try:
            async for chunk in llm_manager.llm.astream(prompt_rag):
                final_answer += chunk
                yield chunk
        except Exception as e:
            print("‚ùå Error while streaming:", e)
        finally:
            try:
                await finalize_rag()
            except Exception as e:
                print("‚ùå Error in finalize_rag:", e)
        print("Finished streaming rag response")

        # üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤ user ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        # form_url, name_form = match_form_from_keywords(user_question)
        # if form_url:
        #     link_text = f"\n\nüìé ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: ‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÉ‡∏ö{markdown_link_from_url(form_url, label=name_form)}\n\n"

        #     final_answer += link_text
        #     yield link_text

    async def finalize_rag():
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô memory
        memory.chat_memory.add_user_message(user_question)
        memory.chat_memory.add_ai_message(final_answer)

        history_now = memory.load_memory_variables({})["history"]

        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏∏‡∏î trigger ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
        if len(history_now) >= SUMMARY_TRIGGER_LEN:
            to_summarize = history_now[:-2]
            new_summary = await summarize_history(to_summarize)

            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á summary store
            memory_manager.set_summary(
                user_id=user_id,
                chat_session_id=chat_session_id,
                mode=mode,
                summary=new_summary
            )

            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï SystemMessage ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô summary ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏•‡∏ö history ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            msgs = memory.chat_memory.messages
            summary_index = next(
                (i for i, m in enumerate(msgs) if isinstance(m, SystemMessage) and m.content.startswith("‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")),
                None
            )

            if summary_index is not None:
                msgs[summary_index] = SystemMessage(content=f"‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤:\n{new_summary}")
            else:
                msgs.insert(0, SystemMessage(content=f"‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤:\n{new_summary}"))

            memory.chat_memory.messages = msgs

    #         his_to = memory.load_memory_variables({})["history"]
    #         print('‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ with RAG : ', his_to)

    # print('summary-rag-memory : ', summary)
    # print('history-rag-memory : ', history)
    # print('instance memory : ',id(memory_stores[user_id]["rag"]))
    # print('‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ ‡∏°‡∏µ RAG : ', history_str)
    return StreamingResponse(stream_rag(), media_type="text/plain")