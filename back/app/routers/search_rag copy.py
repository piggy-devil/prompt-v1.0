from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from typing import Literal, Optional
import uuid
# from app.routers.search import perform_search
from app.managers.local_llm_manager import llm_manager
from fastapi import APIRouter, HTTPException
from app.managers.db_manager import db_manager, get_unique_parent_id_with_score, search_chunk_by_parent_id

router = APIRouter()

# ------------------------------
# Config
# ------------------------------
SUMMARY_TRIGGER_LEN = 12  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô message ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡∏∏‡∏õ
MEMORY_WINDOW = 12  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô message ‡∏ó‡∏µ‡πà‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô memory

# ------------------------------
# In-memory stores
# ------------------------------
# memory_store: dict[str, dict[str, ConversationBufferWindowMemory]] = {}
# memory_stores: dict[str, dict[str, ConversationBufferWindowMemory]] = {}
# summary_store: dict[str, dict[str, str]] = {}  # summary_store[user_id][mode] = str

# ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á: memory_stores[user_id][chat_session_id][mode] = memory
memory_stores: dict[str, dict[str, dict[str, ConversationBufferWindowMemory]]] = {}
# ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á: summary_store[user_id][chat_session_id][mode] = summary_str
summary_store: dict[str, dict[str, dict[str, str]]] = {}

# ------------------------------
# Request model
# ------------------------------
class SearchRequest(BaseModel):
    prompt: str
    # collection_name: str = "thai_law_hybrid"
    l_search: int = 15
    l_chunk: int = 20
    model: str | None = "llama3.2"
    user_id: str | None = None
    chat_session_id: str | None = None

# ------------------------------
# Memory functions
# ------------------------------

# --------- memory ‡∏´‡∏•‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ (‡πÉ‡∏ô-memory store) ---------
# def get_memory(user_id: str) -> ConversationBufferWindowMemory:
#     if user_id not in memory_store:
#         memory_store[user_id] = ConversationBufferWindowMemory(k=5, return_messages=True)
#     return memory_store[user_id]

# def get_memories(user_id: str, mode: Literal["general", "welfare", "law", "unknown"]) -> ConversationBufferWindowMemory:
#     if user_id not in memory_stores:
#         memory_stores[user_id] = {}
#     if mode not in memory_stores[user_id]:
#         memory_stores[user_id][mode] = ConversationBufferWindowMemory(k=MEMORY_WINDOW, return_messages=True)
#     return memory_stores[user_id][mode]

# def get_summary(user_id: str, mode: str) -> str:
#     return summary_store.get(user_id, {}).get(mode, "")

# def set_summary(user_id: str, mode: str, summary: str):
#     if user_id not in summary_store:
#         summary_store[user_id] = {}
#     summary_store[user_id][mode] = summary

def get_memories(
    user_id: str,
    chat_session_id: str,
    mode: Literal["general", "welfare", "law", "unknown"]
) -> ConversationBufferWindowMemory:
    if user_id not in memory_stores:
        memory_stores[user_id] = {}
    if chat_session_id not in memory_stores[user_id]:
        memory_stores[user_id][chat_session_id] = {}
    if mode not in memory_stores[user_id][chat_session_id]:
        memory_stores[user_id][chat_session_id][mode] = ConversationBufferWindowMemory(
            k=MEMORY_WINDOW,
            return_messages=True
        )
    return memory_stores[user_id][chat_session_id][mode]

def get_summary(user_id: str, chat_session_id: str, mode: str) -> str:
    return summary_store.get(user_id, {}).get(chat_session_id, {}).get(mode, "")

def set_summary(user_id: str, chat_session_id: str, mode: str, summary: str):
    if user_id not in summary_store:
        summary_store[user_id] = {}
    if chat_session_id not in summary_store[user_id]:
        summary_store[user_id][chat_session_id] = {}
    summary_store[user_id][chat_session_id][mode] = summary

def set_prompt_expert(expert: str, context: str, history_str: str, user_question: str):
    prompt_rag = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô{expert} ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å{expert}‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô

‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:
{context}

‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:
{history_str}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {user_question}
‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:

‡∏´‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ç‡∏≠ "‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°" ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô RAG ‡πÄ‡∏ä‡πà‡∏ô ‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏•‡∏≤‡∏û‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏ô ‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏•‡∏≤‡∏Å‡∏¥‡∏à ‡∏Ø‡∏•‡∏Ø  
‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Markdown link ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô:

üìé [‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏•‡∏≤‡∏û‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏ô](https://yourdomain.com/forms/leave_form_vacation.pdf)

‡∏≠‡∏¢‡πà‡∏≤‡πÅ‡∏™‡∏î‡∏á URL ‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô `https://...` ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏ä‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÉ‡∏î‡πÜ ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô url ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡πÄ‡∏à‡∏≠‡πÄ‡∏•‡∏¢
"""
    return prompt_rag

# ------------------------------
# Dummy classify function
# ------------------------------
GREETING_KEYWORDS = ["‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", "‡∏ö‡∏≤‡∏¢", "‡∏ó‡∏î‡∏™‡∏≠‡∏ö", "hello", "hi", "‡∏Ñ‡∏∏‡∏ì‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£"]

def is_general_question(prompt: str) -> bool:
    return any(word in prompt.lower() for word in GREETING_KEYWORDS)

async def classify_question_type(prompt: str) -> Literal["‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ", "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢", "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£", "‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°"]:
    
    if (is_general_question(prompt)):
        return "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"
    
    few_shot_prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡πâ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (RAG)

‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏´‡∏°‡∏ß‡∏î‡πÉ‡∏î:

- "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ" = ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢, ‡∏ñ‡∏≤‡∏°‡∏™‡∏≤‡∏£‡∏ó‡∏∏‡∏Å‡∏Ç‡πå, ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÄ‡∏ä‡πà‡∏ô "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏î‡∏µ‡πÑ‡∏´‡∏°", "‡∏Ñ‡∏∏‡∏ì‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£"
- "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£" = ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏ù‡πà‡∏≤‡∏¢‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• ‡πÄ‡∏ä‡πà‡∏ô ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£, ‡∏Ñ‡πà‡∏≤‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏Ç‡∏≠‡∏á‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô, ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå, ‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏π‡πâ, ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡∏•‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÜ, ‡πÄ‡∏Å‡∏©‡∏µ‡∏¢‡∏ì‡∏≠‡∏≤‡∏¢‡∏∏, ‡πÄ‡∏á‡∏¥‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠, ‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏û‡∏¥‡πÄ‡∏®‡∏©, ‡πÄ‡∏ö‡∏¥‡∏Å‡∏Ñ‡πà‡∏≤‡∏£‡∏±‡∏Å‡∏©‡∏≤, ‡∏Å‡∏≤‡∏£‡∏•‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏á‡∏≤‡∏ô, ‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ß‡∏∏‡∏í‡∏¥, ‡∏ì‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à ‡∏Ø‡∏•‡∏Ø
- "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢" = ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢ ‡∏°‡∏≤‡∏ï‡∏£‡∏≤ ‡∏Ç‡πâ‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡∏û‡∏£‡∏ö ‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥ ‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏à‡∏±‡∏î‡∏à‡πâ‡∏≤‡∏á ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡∏Ø‡∏•‡∏Ø
- "‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°" = ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏° ‡πÄ‡∏ä‡πà‡∏ô ‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏ß‡∏±‡∏ô‡∏•‡∏≤, ‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ö‡∏¥‡∏Å, ‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏Ñ‡πà‡∏≤‡∏£‡∏±‡∏Å‡∏©‡∏≤ ‡∏Ø‡∏•‡∏Ø

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö  
‡∏´‡∏°‡∏ß‡∏î: ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: ‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏à‡∏±‡∏î‡∏à‡πâ‡∏≤‡∏á  
‡∏´‡∏°‡∏ß‡∏î: ‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: ‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥  
‡∏´‡∏°‡∏ß‡∏î: ‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: ‡∏•‡∏≤‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡∏µ‡πà‡∏ß‡∏±‡∏ô  
‡∏´‡∏°‡∏ß‡∏î: ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: ‡∏Ç‡∏≠‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏ß‡∏±‡∏ô‡∏•‡∏≤  
‡∏´‡∏°‡∏ß‡∏î: ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£

‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏Ñ‡πà: ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£:

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {prompt}  
‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:"""

    result = ""
    async for chunk in llm_manager.llm.astream(few_shot_prompt):
        result += chunk

    if "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ" in result:
        return "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"
    elif "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£" in result:
        return "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£"
    else:
        return "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢"


# ------------------------------
# Summarization function
# ------------------------------
async def summarize_history(history: list) -> str:
    conversation = "\n".join([
        f"User: {msg.content}" if isinstance(msg, HumanMessage) else f"AI: {msg.content}"
        for msg in history
    ])
    prompt = f"""‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö:\n\n{conversation}\n\n‡∏™‡∏£‡∏∏‡∏õ:"""

    print('Prompt : ', prompt)
    
    summary = ""
    async for chunk in llm_manager.llm.astream(prompt):
        summary += chunk

    print('\nSummary : ', summary.strip())

    return summary.strip()

# ------------------------------
# Perform Search function
# ------------------------------
async def perform_search(prompt: str, collection_name: str, l_search: int, l_chunk: int):
    try:
        vectorstore = db_manager.get_vectorstore(collection_name)

        # Step 1: ‡∏î‡∏∂‡∏á top 15 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏≤‡∏à‡∏°‡∏µ doc ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥ ‡∏î‡∏∂‡∏á‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô
        result_with_score = vectorstore.similarity_search_with_score(prompt, k=l_search)
        # Step 2: ‡∏Ñ‡∏±‡∏î doc ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô (l_search) ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô ‡∏Å‡∏£‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ parent_id
        unique_docs = get_unique_parent_id_with_score(result_with_score, limit=l_search)

        contexts = []
        for parent_id, chunk in unique_docs.items():
            full_content = search_chunk_by_parent_id(collection_name=collection_name, parent_id=parent_id, limit=l_chunk)
            score = chunk[0][1]
            contexts.append({
                "parent_id": parent_id,
                "score": score,
                "content": full_content
            })

        # print(db_manager.get_embedding("dense").embed_query("‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"))

        return contexts

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {str(e)}"
        )

# ------------------------------
# Match Form function
# ------------------------------    
def match_form_from_keywords(prompt: str) -> Optional[str]:
    prompt = prompt.lower()
    forms = {
        "‡∏•‡∏≤‡∏û‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏ô": "forms/leave_form_vacation.pdf",
        "‡∏•‡∏≤‡∏Å‡∏¥‡∏à": "forms/leave_form_personal.pdf",
        "‡∏•‡∏≤‡∏õ‡πà‡∏ß‡∏¢": "forms/leave_form_sick.pdf",
        "‡∏•‡∏≤‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô": "forms/leave_form_sick.pdf"
    }
    for keyword, path in forms.items():
        if keyword in prompt and ("‡πÅ‡∏ö‡∏ö" in prompt or "‡∏ü‡∏≠‡∏£‡πå‡∏°" in prompt):
            return f"http://localhost:8000/{path}", keyword
    return None

def markdown_link_from_url(url: str, label: str = "‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î") -> str:
    return f"[{label}]({url})"


# ------------------------------
# Main endpoint
# ------------------------------
@router.post("/llm/search-rag/chat")
async def search_endpoint(request: SearchRequest):
    if request.model:
        llm_manager.set_model(request.model)

    user_id = request.user_id or str(uuid.uuid4())  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î user id ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á‡∏°‡∏≤
    chat_session_id = request.chat_session_id or str(uuid.uuid4())  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î chat_session_id ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á‡∏°‡∏≤
    user_question = request.prompt

    # ------- Classify ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô ------
    category = await classify_question_type(user_question)

    print("‡∏´‡∏°‡∏ß‡∏î : ", category)

    # ‡πÅ‡∏¢‡∏Å memory
    if category == "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ":
        mode = "general"
    elif category == "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£":
        mode = "welfare"
    elif category == "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢":
        mode = "law"
    else:
        mode = "unknown"

    # category = "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"
    
    # memory = get_memory(user_id)
    memory = get_memories(user_id, chat_session_id, mode)

    # Load history & summary
    summary = get_summary(user_id, chat_session_id, mode)
    history = memory.load_memory_variables({})["history"]

    # ------- Load ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å memory ------
    history_str = ""
    for msg in history:
        if isinstance(msg, HumanMessage):
            history_str += f"User: {msg.content}\n"
        elif isinstance(msg, AIMessage):
            history_str += f"AI: {msg.content}\n"

    # ------- ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ RAG) ------
    if category == "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ":
        # print('summary-general-memory : ', summary)
        # print('history-general-memory : ', history)
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏†‡∏≤‡∏û\n\n‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:\n\n{history_str}\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {user_question}
\n\n‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:"""
        full_response = ""
        async def stream_general():
            nonlocal full_response
            
            async for chunk in llm_manager.llm.astream(prompt):
                full_response += chunk
                yield chunk

            await finalize()

        async def finalize():
            memory.chat_memory.add_user_message(user_question)
            memory.chat_memory.add_ai_message(full_response)

            history_now = memory.load_memory_variables({})["history"]
            if len(history_now) >= SUMMARY_TRIGGER_LEN:
                to_summarize = history_now[:-2]
                new_summary = await summarize_history(to_summarize)
                set_summary(user_id, mode, new_summary)

                memory.chat_memory.messages = [
                    SystemMessage(content=f"‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤:\n{new_summary}"),
                    history_now[-2],
                    history_now[-1],
                ]
                # his_to = memory.load_memory_variables({})["history"]
                # print('‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ : ', his_to)

        # print('instance memory : ',id(memory_stores[user_id]["general"]))
        # print('‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ RAG : ', history_str)
        return StreamingResponse(stream_general(), media_type="text/plain")

    # ------- ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡πÉ‡∏ä‡πâ RAG) -------

    if (category == "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢") :
        collection_name = "thai_law_hybrid"
    elif (category == "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£"):
        collection_name = "welfare"
    else:
        collection_name = "unknown"
        
    data_from_rag = await perform_search(
        prompt=request.prompt, 
        collection_name=collection_name, 
        l_search=request.l_search, 
        l_chunk=request.l_chunk
    )
    filtered_contents = [item["content"] for item in data_from_rag if item["score"] > 0.2]
    context = "\n".join(filtered_contents)

    prompt_rag = set_prompt_expert(category, context, history_str, user_question)

    final_answer = ""
    async def stream_rag():
        nonlocal final_answer
        async for chunk in llm_manager.llm.astream(prompt_rag):
            final_answer += chunk
            yield chunk

        # üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤ user ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        # form_url, name_form = match_form_from_keywords(user_question)
        # if form_url:
        #     link_text = f"\n\nüìé ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: ‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÉ‡∏ö{markdown_link_from_url(form_url, label=name_form)}\n\n"

        #     final_answer += link_text
        #     yield link_text

        await finalize_rag()

    async def finalize_rag():
        memory.chat_memory.add_user_message(user_question)
        memory.chat_memory.add_ai_message(final_answer)

        history_now = memory.load_memory_variables({})["history"]
        if len(history_now) >= SUMMARY_TRIGGER_LEN:
            to_summarize = history_now[:-2]
            new_summary = await summarize_history(to_summarize)
            set_summary(user_id, mode, new_summary)

            memory.chat_memory.messages = [
                SystemMessage(content=f"‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤:\n{new_summary}"),
                history_now[-2],
                history_now[-1],
            ]
    #         his_to = memory.load_memory_variables({})["history"]
    #         print('‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ with RAG : ', his_to)

    # print('summary-rag-memory : ', summary)
    # print('history-rag-memory : ', history)
    # print('instance memory : ',id(memory_stores[user_id]["rag"]))
    # print('‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ ‡∏°‡∏µ RAG : ', history_str)
    return StreamingResponse(stream_rag(), media_type="text/plain")